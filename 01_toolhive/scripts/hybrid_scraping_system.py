#!/usr/bin/env python3
"""
ToolHive + Python í•˜ì´ë¸Œë¦¬ë“œ ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ

Desktop Commanderì˜ MCP ë¸Œë¼ìš°ì € ê¸°ëŠ¥ê³¼ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¡°í•©í•˜ì—¬
ëŒ€ê·œëª¨ ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ì´ ì‹œìŠ¤í…œì€ ToolHiveì˜ MCP ê¸°ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•˜ë©´ì„œë„
ì‹¤ìš©ì ì¸ ìŠ¤í¬ë˜í•‘ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import json
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import requests
from bs4 import BeautifulSoup

@dataclass
class ScrapingTarget:
    """ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ì‚¬ì´íŠ¸ ì •ë³´"""
    name: str
    url: str
    title_selector: str = "title"
    description: str = ""

@dataclass 
class ScrapingResult:
    """ìŠ¤í¬ë˜í•‘ ê²°ê³¼"""
    target: ScrapingTarget
    title: Optional[str] = None
    content: Optional[str] = None
    error: Optional[str] = None
    timestamp: Optional[str] = None

class ToolHiveScrapingSystem:
    """ToolHive MCP + Python í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.results: List[ScrapingResult] = []
        self.mcp_available = False
        
    def check_mcp_availability(self) -> bool:
        """MCP ë¸Œë¼ìš°ì € ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            # Desktop Commander MCP ë¸Œë¼ìš°ì € ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” MCP í•¨ìˆ˜ í˜¸ì¶œ ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸
            print("ğŸ” MCP ë¸Œë¼ìš°ì € ê¸°ëŠ¥ í™•ì¸ ì¤‘...")
            # í˜„ì¬ëŠ” ì§ì ‘ ì ‘ê·¼ì´ ì–´ë ¤ìš°ë¯€ë¡œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            self.mcp_available = False
            print("âš ï¸ MCP ì§ì ‘ ì ‘ê·¼ ë¶ˆê°€, Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í´ë°±")
            return False
        except Exception as e:
            print(f"âŒ MCP ë¸Œë¼ìš°ì € ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€: {e}")
            self.mcp_available = False
            return False
    
    def scrape_with_mcp(self, target: ScrapingTarget) -> ScrapingResult:
        """Desktop Commander MCP ë¸Œë¼ìš°ì € ê¸°ëŠ¥ì„ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸŒ MCPë¡œ ìŠ¤í¬ë˜í•‘: {target.name} ({target.url})")
        
        try:
            # ì‹¤ì œ MCP ë¸Œë¼ìš°ì € ê¸°ëŠ¥ í˜¸ì¶œ
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
            
            # MCP ë¸Œë¼ìš°ì €ë¡œ í˜ì´ì§€ ì´ë™
            # mcp_playwright_browser_navigate(url=target.url)
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ ê²°ê³¼ (ì‹¤ì œë¡œëŠ” MCPì—ì„œ ë°›ì€ ë°ì´í„°)
            simulated_mcp_result = {
                "title": "í´ë˜ìŠ¤ìœ  (2025) | ì„¸ìƒ ëª¨ë“  ë°°ì›€ ì´ˆíŠ¹ê°€!!!",
                "url": target.url,
                "content": "ì‹œë®¬ë ˆì´ì…˜ëœ MCP ê²°ê³¼"
            }
            
            return ScrapingResult(
                target=target,
                title=simulated_mcp_result.get("title"),
                content=simulated_mcp_result.get("content"),
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
            )
            
        except Exception as e:
            return ScrapingResult(
                target=target,
                error=f"MCP ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}",
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
            )
    
    def scrape_with_requests(self, target: ScrapingTarget) -> ScrapingResult:
        """Python requests + BeautifulSoupì„ ì‚¬ìš©í•œ í´ë°± ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸ Pythonìœ¼ë¡œ ìŠ¤í¬ë˜í•‘: {target.name} ({target.url})")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(target.url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title_element = soup.select_one(target.title_selector)
            title = title_element.get_text().strip() if title_element else None
            
            return ScrapingResult(
                target=target,
                title=title,
                content=response.text[:1000],  # ì²˜ìŒ 1000ìë§Œ ì €ì¥
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
            )
            
        except Exception as e:
            return ScrapingResult(
                target=target,
                error=f"Python ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}",
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
            )
    
    def scrape_single(self, target: ScrapingTarget) -> ScrapingResult:
        """ë‹¨ì¼ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ (MCP ìš°ì„ , ì‹¤íŒ¨ì‹œ Python í´ë°±)"""
        print(f"\nğŸ¯ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {target.name}")
        
        # MCP ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ìš°ì„  ì‹œë„
        if self.mcp_available:
            result = self.scrape_with_mcp(target)
            if not result.error:
                print(f"âœ… MCP ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {result.title}")
                return result
            else:
                print(f"âš ï¸ MCP ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨, Pythonìœ¼ë¡œ í´ë°±")
        
        # Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í´ë°±
        result = self.scrape_with_requests(target)
        if not result.error:
            print(f"âœ… Python ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {result.title}")
        else:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì™„ì „ ì‹¤íŒ¨: {result.error}")
        
        return result
    
    def scrape_multiple(self, targets: List[ScrapingTarget]) -> List[ScrapingResult]:
        """ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
        print(f"ğŸš€ ëŒ€ê·œëª¨ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {len(targets)}ê°œ ì‚¬ì´íŠ¸")
        
        results = []
        for i, target in enumerate(targets, 1):
            print(f"\nğŸ“Š ì§„í–‰ë¥ : {i}/{len(targets)}")
            result = self.scrape_single(target)
            results.append(result)
            self.results.append(result)
            
            # ìš”ì²­ ê°„ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            if i < len(targets):
                time.sleep(1)
        
        return results
    
    def save_results(self, filename: str = "scraping_results.json"):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            results_data = []
            for result in self.results:
                results_data.append({
                    "target_name": result.target.name,
                    "target_url": result.target.url,
                    "title": result.title,
                    "error": result.error,
                    "timestamp": result.timestamp
                })
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def print_summary(self):
        """ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“ˆ === ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìš”ì•½ ===")
        print(f"ì´ ëŒ€ìƒ: {len(self.results)}ê°œ")
        
        successful = [r for r in self.results if not r.error and r.title]
        failed = [r for r in self.results if r.error]
        
        print(f"ì„±ê³µ: {len(successful)}ê°œ")
        print(f"ì‹¤íŒ¨: {len(failed)}ê°œ")
        
        if successful:
            print(f"\nâœ… ì„±ê³µí•œ ì‚¬ì´íŠ¸ë“¤:")
            for result in successful:
                print(f"  - {result.target.name}: {result.title}")
        
        if failed:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ì‚¬ì´íŠ¸ë“¤:")
            for result in failed:
                print(f"  - {result.target.name}: {result.error}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ToolHive í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ ì‹œì‘")
    
    # ìŠ¤í¬ë˜í•‘ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    scraper = ToolHiveScrapingSystem()
    scraper.check_mcp_availability()
    
    # ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ì‚¬ì´íŠ¸ë“¤ ì •ì˜
    targets = [
        ScrapingTarget(
            name="í´ë˜ìŠ¤ìœ ",
            url="https://www.classu.co.kr/new",
            description="ì˜¨ë¼ì¸ í´ë˜ìŠ¤ í”Œë«í¼"
        ),
        ScrapingTarget(
            name="ë„¤ì´ë²„",
            url="https://www.naver.com",
            description="í¬í„¸ ì‚¬ì´íŠ¸"
        ),
        ScrapingTarget(
            name="GitHub",
            url="https://github.com",
            description="ê°œë°œì í”Œë«í¼"
        ),
        ScrapingTarget(
            name="Stack Overflow",
            url="https://stackoverflow.com",
            description="ê°œë°œì Q&A"
        )
    ]
    
    # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    results = scraper.scrape_multiple(targets)
    
    # ê²°ê³¼ ìš”ì•½ ë° ì €ì¥
    scraper.print_summary()
    scraper.save_results("toolhive_scraping_results.json")
    
    print(f"\nğŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: toolhive_scraping_results.json")

if __name__ == "__main__":
    main()